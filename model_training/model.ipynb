{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:46:49.783870Z",
     "iopub.status.busy": "2025-07-14T15:46:49.783714Z",
     "iopub.status.idle": "2025-07-14T15:46:56.703688Z",
     "shell.execute_reply": "2025-07-14T15:46:56.702771Z",
     "shell.execute_reply.started": "2025-07-14T15:46:49.783855Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "bigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.9.0.13 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.4.0.6 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.10.19 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.4.40 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.9.5 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.9.41 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets evaluate rouge_score --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T15:47:04.121683Z",
     "iopub.status.busy": "2025-07-14T15:47:04.121118Z",
     "iopub.status.idle": "2025-07-14T18:01:01.874126Z",
     "shell.execute_reply": "2025-07-14T18:01:01.873428Z",
     "shell.execute_reply.started": "2025-07-14T15:47:04.121646Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 15:47:17.839343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752508038.041844      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752508038.103341      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9c31dc2b2540e3a935f3c1236eae3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d2ddbb3e5a48bf999444b90ebb08c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1aeae2586074677a8e7b6ccabc00e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f300b6fab2479ea6473946523cf140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93f9250d5c6842eeb9a453ac38996751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/3248771115.py:73: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/3:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/3248771115.py:100: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: ./t5_summarizer/checkpoint_epoch_1.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.6422\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/3:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: ./t5_summarizer/checkpoint_epoch_2.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.5500\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/3:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved checkpoint: ./t5_summarizer/checkpoint_epoch_3.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.5326\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import AdamW\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"t5-base\"\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 150\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "EVAL_BATCH_SIZE = 4\n",
    "NUM_EPOCHS = 3\n",
    "LEARNING_RATE = 3e-5\n",
    "WEIGHT_DECAY = 0.01\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WARMUP_STEPS = 500\n",
    "OUTPUT_DIR = \"./t5_summarizer\"\n",
    "\n",
    "data_path = \"/kaggle/input/ccdv-arxiv-summarization-dataset/train.csv\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME).to(device)\n",
    "\n",
    "class ArxivCSV(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.articles = df[\"article\"].tolist()\n",
    "        self.abstracts = df[\"abstract\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = str(self.articles[idx])\n",
    "        target = str(self.abstracts[idx])\n",
    "        inputs = tokenizer(\n",
    "            source,\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        labels = tokenizer(\n",
    "            target,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        input_ids = torch.tensor(inputs.input_ids)\n",
    "        attention_mask = torch.tensor(inputs.attention_mask)\n",
    "        labels_ids = torch.tensor(labels.input_ids)\n",
    "        labels_ids[labels_ids == tokenizer.pad_token_id] = -100\n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels_ids}\n",
    "\n",
    "# Read CSV and split\n",
    "df = pd.read_csv(data_path)\n",
    "train_df = df.iloc[:20000].reset_index(drop=True)\n",
    "val_df = df.iloc[20000:25000].reset_index(drop=True)\n",
    "\n",
    "dataset_train = ArxivCSV(train_df)\n",
    "dataset_val = ArxivCSV(val_df)\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=EVAL_BATCH_SIZE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "total_steps = len(train_loader) // GRADIENT_ACCUMULATION_STEPS * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=total_steps)\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    eval_loader = tqdm(val_loader, desc=\"Validating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            total_loss += outputs.loss.item()\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    for epoch in range(1, NUM_EPOCHS + 1):\n",
    "        running_loss = 0.0\n",
    "        epoch_loader = tqdm(train_loader, desc=f\"Epoch {epoch}/{NUM_EPOCHS}\", leave=False)\n",
    "        for step, batch in enumerate(epoch_loader, start=1):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss / GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if step % GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "\n",
    "                epoch_loader.set_postfix({\"loss\": f\"{running_loss/GRADIENT_ACCUMULATION_STEPS:.4f}\"})\n",
    "                running_loss = 0.0\n",
    "\n",
    "        # Save checkpoint\n",
    "        ckpt_path = os.path.join(OUTPUT_DIR, f\"checkpoint_epoch_{epoch}.pt\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"Saved checkpoint: {ckpt_path}\")\n",
    "\n",
    "        # Run validation after each epoch\n",
    "        evaluate()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T16:58:39.958307Z",
     "iopub.status.busy": "2025-07-13T16:58:39.957766Z",
     "iopub.status.idle": "2025-07-13T17:09:29.199347Z",
     "shell.execute_reply": "2025-07-13T17:09:29.198636Z",
     "shell.execute_reply.started": "2025-07-13T16:58:39.958282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "from evaluate import load as load_metric\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Config\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "MODEL_NAME = \"t5-base\"\n",
    "CHECKPOINT_PATH = \"./t5_summarizer/checkpoint_epoch_3.pt\"\n",
    "DATA_PATH = \"/kaggle/input/ccdv-arxiv-summarization-dataset/train.csv\"\n",
    "MAX_INPUT_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 150\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# Load tokenizer and model\n",
    "print(\"Loading tokenizer and model...\")\n",
    "tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# Load validation data\n",
    "print(\"Loading validation data...\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "val_df = df.iloc[20000:20500].reset_index(drop=True)\n",
    "\n",
    "class ArxivValDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.articles = df[\"article\"].tolist()\n",
    "        self.abstracts = df[\"abstract\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source = str(self.articles[idx])\n",
    "        target = str(self.abstracts[idx])\n",
    "        inputs = tokenizer(\n",
    "            source,\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": inputs.input_ids.squeeze(0),\n",
    "            \"attention_mask\": inputs.attention_mask.squeeze(0),\n",
    "            \"target\": target\n",
    "        }\n",
    "\n",
    "val_dataset = ArxivValDataset(val_df)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load ROUGE\n",
    "print(\"Loading ROUGE metric...\")\n",
    "rouge = load_metric(\"rouge\")\n",
    "\n",
    "\n",
    "# Generate summaries and compute ROUGE\n",
    "print(\"Evaluating...\")\n",
    "preds = []\n",
    "refs = []\n",
    "\n",
    "for batch in tqdm(val_loader):\n",
    "    input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "    attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        summaries = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            num_beams=4,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(summaries, skip_special_tokens=True)\n",
    "    decoded_refs = [ref for ref in batch[\"target\"]]\n",
    "\n",
    "    preds.extend(decoded_preds)\n",
    "    refs.extend(decoded_refs)\n",
    "\n",
    "# Compute ROUGE\n",
    "print(\"\\nComputing ROUGE scores...\")\n",
    "rouge_output = rouge.compute(predictions=preds, references=refs, use_stemmer=True)\n",
    "\n",
    "for key in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "    score = rouge_output[key]\n",
    "    print(f\"{key.upper()} F1: {score:.4f}\")\n",
    "\n",
    "\n",
    "# Sample Input and Summary\n",
    "print(\"\\nğŸ” Sample Result:\")\n",
    "sample_input = val_df.iloc[0][\"article\"][:1000]  # Shortened for readability\n",
    "input_tokens = tokenizer.encode(sample_input, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True).to(DEVICE)\n",
    "summary_ids = model.generate(input_tokens, max_length=MAX_TARGET_LENGTH)\n",
    "sample_output = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nğŸ“ Input (truncated):\\n\", sample_input[:500], \"...\\n\")\n",
    "print(\"ğŸ“Œ Model Summary:\\n\", sample_output)\n",
    "print(\"ğŸ¯ Reference Summary:\\n\", val_df.iloc[0][\"abstract\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:01:18.807079Z",
     "iopub.status.busy": "2025-07-14T18:01:18.806606Z",
     "iopub.status.idle": "2025-07-14T18:01:23.308084Z",
     "shell.execute_reply": "2025-07-14T18:01:23.307479Z",
     "shell.execute_reply.started": "2025-07-14T18:01:18.807057Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary:\n",
      " Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding. however, their deployment in real-world applications is limited by issues such as high inference latency and large memory requirements. we introduce a two-stage approach combining knowledge distillation with safety-aligned reinforcement learning to produce compact, safe, and efficient LLMs.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "# === Config ===\n",
    "MODEL_NAME = \"t5-base\"\n",
    "CHECKPOINT_PATH = \"./t5_summarizer/checkpoint_epoch_3.pt\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# === Load tokenizer and model ===\n",
    "tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "# === Input Text (sample) ===\n",
    "input_text = \"\"\"\n",
    "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation. However, their deployment in real-world applications is limited by issues such as high inference latency, large memory requirements, and the risk of generating factually incorrect or toxic content. In this work, we introduce a two-stage approach combining knowledge distillation with safety-aligned reinforcement learning to produce compact, safe, and efficient LLMs. We evaluate our models on a suite of benchmark tasks including summarization, question answering, and factual correctness, and show that our method significantly reduces model size and latency while maintaining competitive accuracy and safety metrics.\n",
    "\"\"\"\n",
    "\n",
    "# === Add 'summarize:' prefix ===\n",
    "prefixed_text = \"summarize: \" + input_text.strip()\n",
    "\n",
    "# === Tokenize input ===\n",
    "inputs = tokenizer(\n",
    "    prefixed_text,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=512\n",
    ").to(DEVICE)\n",
    "\n",
    "# === Generate Summary ===\n",
    "summary_ids = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=256,                   # Better than 150 for full thought\n",
    "    num_beams=4,                      # Beam search for quality\n",
    "    repetition_penalty=2.0,           # Avoid repeated phrases\n",
    "    no_repeat_ngram_size=4,           # Block repeated n-grams\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# === Decode and Post-process Summary ===\n",
    "summary = tokenizer.decode(\n",
    "    summary_ids[0],\n",
    "    skip_special_tokens=True,\n",
    "    clean_up_tokenization_spaces=True\n",
    ")\n",
    "\n",
    "# Optional: Capitalize the first letter\n",
    "summary = summary[0].upper() + summary[1:]\n",
    "\n",
    "print(\"Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:03:05.461689Z",
     "iopub.status.busy": "2025-07-14T18:03:05.461163Z",
     "iopub.status.idle": "2025-07-14T18:03:09.205449Z",
     "shell.execute_reply": "2025-07-14T18:03:09.204866Z",
     "shell.execute_reply.started": "2025-07-14T18:03:05.461665Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('my_local_t5_model/tokenizer_config.json',\n",
       " 'my_local_t5_model/special_tokens_map.json',\n",
       " 'my_local_t5_model/spiece.model',\n",
       " 'my_local_t5_model/added_tokens.json',\n",
       " 'my_local_t5_model/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.load_state_dict(torch.load(\"./t5_summarizer/checkpoint_epoch_3.pt\"))\n",
    "\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "\n",
    "# Save both into a folder\n",
    "model.save_pretrained(\"my_local_t5_model\")\n",
    "tokenizer.save_pretrained(\"my_local_t5_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-13T17:49:44.465824Z",
     "iopub.status.busy": "2025-07-13T17:49:44.465572Z",
     "iopub.status.idle": "2025-07-13T17:50:30.932039Z",
     "shell.execute_reply": "2025-07-13T17:50:30.931357Z",
     "shell.execute_reply.started": "2025-07-13T17:49:44.465807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\"my_local_t5_model\", 'zip', \"my_local_t5_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:03:36.271999Z",
     "iopub.status.busy": "2025-07-14T18:03:36.271716Z",
     "iopub.status.idle": "2025-07-14T18:04:26.841682Z",
     "shell.execute_reply": "2025-07-14T18:04:26.840921Z",
     "shell.execute_reply.started": "2025-07-14T18:03:36.271977Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: cannot stat 'my_local_t5_model.zip': No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cp: 'my_local_t5_model.zip' and '/kaggle/working/my_local_t5_model.zip' are the same file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Move the zip file to the output directory so it appears in the Files tab\n",
    "!cp my_local_t5_model.zip /kaggle/working/\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "import shutil\n",
    "\n",
    "# Save model and tokenizer\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.load_state_dict(torch.load(\"./t5_summarizer/checkpoint_epoch_3.pt\"))\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "model.save_pretrained(\"my_local_t5_model\")\n",
    "tokenizer.save_pretrained(\"my_local_t5_model\")\n",
    "\n",
    "# Zip and move to working\n",
    "shutil.make_archive(\"my_local_t5_model\", 'zip', \"my_local_t5_model\")\n",
    "!cp my_local_t5_model.zip /kaggle/working/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-14T18:14:35.963029Z",
     "iopub.status.busy": "2025-07-14T18:14:35.962687Z",
     "iopub.status.idle": "2025-07-14T18:14:48.622136Z",
     "shell.execute_reply": "2025-07-14T18:14:48.621133Z",
     "shell.execute_reply.started": "2025-07-14T18:14:35.963001Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping folder: my_local_t5_model; use '--dir-mode' to upload folders\n",
      "Skipping folder: t5_summarizer; use '--dir-mode' to upload folders\n",
      "Skipping folder: .virtual_documents; use '--dir-mode' to upload folders\n",
      "Starting upload for file my_local_t5_model.zip\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782M/782M [00:04<00:00, 167MB/s]\n",
      "Upload successful: my_local_t5_model.zip (782MB)\n",
      "Your public Dataset is being created. Please check progress at https://www.kaggle.com/datasets/kanishk2223/my-t5-model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "model.load_state_dict(torch.load(\"./t5_summarizer/checkpoint_epoch_3.pt\"))\n",
    "tokenizer = T5TokenizerFast.from_pretrained(\"t5-base\")\n",
    "\n",
    "SAVE_DIR = \"/kaggle/working/my_local_t5_model\"\n",
    "model.save_pretrained(SAVE_DIR)\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"Your_kaggle_username_here\"  # Replace with your actual Kaggle username\n",
    "os.environ[\"KAGGLE_KEY\"] = \"Your_kaggle_key_here\"  # Replace with your actual Kaggle key download from kaggle account\n",
    "\n",
    "dataset_metadata = {\n",
    "    \"title\": \"My T5 Summarizer Model\",\n",
    "    \"id\": \"kanishk2223/my-t5-model\", \n",
    "    \"licenses\": [{\"name\": \"CC0-1.0\"}]\n",
    "}\n",
    "\n",
    "with open(\"/kaggle/working/dataset-metadata.json\", \"w\") as f:\n",
    "    json.dump(dataset_metadata, f)\n",
    "\n",
    "!kaggle datasets create -p /kaggle/working/ -u\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4115644,
     "sourceId": 7133076,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
